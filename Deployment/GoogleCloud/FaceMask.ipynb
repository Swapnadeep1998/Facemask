{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"face-mask-12k-images-dataset.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"storage/FaceMask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size=128\n",
    "val_batch_size=32\n",
    "\n",
    "\n",
    "train_dir='storage/FaceMask/Face Mask Dataset/Train'\n",
    "val_dir='storage/FaceMask/Face Mask Dataset/Validation'\n",
    "test_dir='storage/FaceMask/Face Mask Dataset/Test'\n",
    "\n",
    "train_datagen=ImageDataGenerator(rescale=1./255,\n",
    "                                rotation_range=40,\n",
    "                                width_shift_range=0.2,\n",
    "                                height_shift_range=0.2,\n",
    "                                shear_range=0.2,\n",
    "                                zoom_range=0.2,\n",
    "                                horizontal_flip=True,\n",
    "                                fill_mode='nearest')\n",
    "val_datagen=ImageDataGenerator(rescale=1./255)\n",
    "test_datagen=ImageDataGenerator(rescale=1./255)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LENET Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n",
      "Found 992 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "79/79 [==============================] - 34s 433ms/step - loss: 0.3984 - accuracy: 0.8338 - val_loss: 0.2883 - val_accuracy: 0.8954\n",
      "Epoch 2/10\n",
      "79/79 [==============================] - 26s 324ms/step - loss: 0.2896 - accuracy: 0.8896 - val_loss: 0.2394 - val_accuracy: 0.9062\n",
      "Epoch 3/10\n",
      "79/79 [==============================] - 24s 310ms/step - loss: 0.2521 - accuracy: 0.8979 - val_loss: 0.1832 - val_accuracy: 0.9435\n",
      "Epoch 4/10\n",
      "79/79 [==============================] - 22s 279ms/step - loss: 0.2391 - accuracy: 0.9064 - val_loss: 0.1610 - val_accuracy: 0.9399\n",
      "Epoch 5/10\n",
      "79/79 [==============================] - 22s 275ms/step - loss: 0.2215 - accuracy: 0.9130 - val_loss: 0.1307 - val_accuracy: 0.9567\n",
      "Epoch 6/10\n",
      "79/79 [==============================] - 22s 275ms/step - loss: 0.2024 - accuracy: 0.9232 - val_loss: 0.1177 - val_accuracy: 0.9651\n",
      "Epoch 7/10\n",
      "79/79 [==============================] - 22s 276ms/step - loss: 0.1896 - accuracy: 0.9272 - val_loss: 0.1117 - val_accuracy: 0.9627\n",
      "Epoch 8/10\n",
      "79/79 [==============================] - 22s 273ms/step - loss: 0.1895 - accuracy: 0.9279 - val_loss: 0.1039 - val_accuracy: 0.9603\n",
      "Epoch 9/10\n",
      "79/79 [==============================] - 22s 276ms/step - loss: 0.1780 - accuracy: 0.9327 - val_loss: 0.1010 - val_accuracy: 0.9675\n",
      "Epoch 10/10\n",
      "79/79 [==============================] - 22s 279ms/step - loss: 0.1716 - accuracy: 0.9367 - val_loss: 0.0960 - val_accuracy: 0.9760\n"
     ]
    }
   ],
   "source": [
    "from CNN_Utils.lenet5 import Lenet_5\n",
    "import PIL\n",
    "\n",
    "epochs=10\n",
    "steps_per_epochs=(10000//batch_size)+1\n",
    "val_steps=(800//val_batch_size)+1\n",
    "\n",
    "\n",
    "train_dataset=train_datagen.flow_from_directory(train_dir,\n",
    "                                               target_size=(32,32),\n",
    "                                               batch_size=batch_size,\n",
    "                                               class_mode='binary')\n",
    "val_dataset=val_datagen.flow_from_directory(val_dir,\n",
    "                                               target_size=(32,32),\n",
    "                                               batch_size=val_batch_size,\n",
    "                                               class_mode='binary')\n",
    "test_dataset=val_datagen.flow_from_directory(test_dir,\n",
    "                                               target_size=(32,32),\n",
    "                                               batch_size=batch_size,\n",
    "                                               class_mode='binary')\n",
    "\n",
    "lenet_model=Lenet_5()\n",
    "\n",
    "lenet_model.fit(train_dataset,\n",
    "               steps_per_epoch=steps_per_epochs,\n",
    "               epochs=epochs,\n",
    "               validation_data=val_dataset,\n",
    "               validation_steps=val_steps,\n",
    "               save_model=True,\n",
    "               model_dir='storage/Trained Models/lenet_facemask.h5')              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALEXNET Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n",
      "Found 992 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "79/79 [==============================] - 90s 1s/step - loss: 0.6707 - accuracy: 0.6796 - val_loss: 0.2618 - val_accuracy: 0.9135\n",
      "Epoch 2/5\n",
      "79/79 [==============================] - 79s 1s/step - loss: 0.2841 - accuracy: 0.8870 - val_loss: 0.1865 - val_accuracy: 0.9459\n",
      "Epoch 3/5\n",
      "79/79 [==============================] - 78s 994ms/step - loss: 0.2385 - accuracy: 0.9110 - val_loss: 0.1725 - val_accuracy: 0.9507\n",
      "Epoch 4/5\n",
      "79/79 [==============================] - 85s 1s/step - loss: 0.2252 - accuracy: 0.9219 - val_loss: 0.1698 - val_accuracy: 0.9399\n",
      "Epoch 5/5\n",
      "79/79 [==============================] - 79s 1s/step - loss: 0.2169 - accuracy: 0.9225 - val_loss: 0.1695 - val_accuracy: 0.9483\n"
     ]
    }
   ],
   "source": [
    "from CNN_Utils.alexnet import AlexNet\n",
    "\n",
    "\n",
    "epochs=5\n",
    "steps_per_epochs=(10000//batch_size)+1\n",
    "validation_steps=(800//val_batch_size)+1\n",
    "\n",
    "\n",
    "train_dataset=train_datagen.flow_from_directory(train_dir,\n",
    "                                               target_size=(150,150),\n",
    "                                               batch_size=batch_size,\n",
    "                                               class_mode='binary')\n",
    "val_dataset=val_datagen.flow_from_directory(val_dir,\n",
    "                                               target_size=(150,150),\n",
    "                                               batch_size=val_batch_size,\n",
    "                                               class_mode='binary')\n",
    "test_dataset=val_datagen.flow_from_directory(test_dir,\n",
    "                                               target_size=(150,150),\n",
    "                                               batch_size=batch_size,\n",
    "                                               class_mode='binary')\n",
    "\n",
    "alexNet_model=AlexNet(shape=(150,150,3))\n",
    "alexNet_model.fit(train_dataset,\n",
    "               steps_per_epoch=steps_per_epochs,\n",
    "               epochs=epochs,\n",
    "               validation_data=val_dataset,\n",
    "               validation_steps=validation_steps,\n",
    "               save_model=True,\n",
    "                 model_dir='storage/Trained Models/facemask_AlexNet.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n",
      "Found 992 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "79/79 [==============================] - 167s 2s/step - loss: 0.2423 - accuracy: 0.9131 - val_loss: 0.0899 - val_accuracy: 0.9808\n",
      "Epoch 2/5\n",
      "79/79 [==============================] - 166s 2s/step - loss: 0.1208 - accuracy: 0.9611 - val_loss: 0.0544 - val_accuracy: 0.9820\n",
      "Epoch 3/5\n",
      "79/79 [==============================] - 166s 2s/step - loss: 0.0969 - accuracy: 0.9683 - val_loss: 0.0503 - val_accuracy: 0.9844\n",
      "Epoch 4/5\n",
      "79/79 [==============================] - 166s 2s/step - loss: 0.0851 - accuracy: 0.9736 - val_loss: 0.0340 - val_accuracy: 0.9916\n",
      "Epoch 5/5\n",
      "79/79 [==============================] - 167s 2s/step - loss: 0.0762 - accuracy: 0.9754 - val_loss: 0.0302 - val_accuracy: 0.9916\n"
     ]
    }
   ],
   "source": [
    "from CNN_Utils.vgg16 import Pretrained_VGG16\n",
    "\n",
    "\n",
    "epochs=5\n",
    "steps_per_epoch=(10000//batch_size)+1\n",
    "validation_steps=(800//val_batch_size)+1\n",
    "\n",
    "\n",
    "train_dataset=train_datagen.flow_from_directory(train_dir,\n",
    "                                               target_size=(150,150),\n",
    "                                               batch_size=batch_size,\n",
    "                                               class_mode='binary')\n",
    "val_dataset=val_datagen.flow_from_directory(val_dir,\n",
    "                                               target_size=(150,150),\n",
    "                                               batch_size=val_batch_size,\n",
    "                                               class_mode='binary')\n",
    "test_dataset=val_datagen.flow_from_directory(test_dir,\n",
    "                                               target_size=(150,150),\n",
    "                                               batch_size=batch_size,\n",
    "                                               class_mode='binary')\n",
    "\n",
    "VGG16_model=Pretrained_VGG16(shape=(150,150,3))\n",
    "VGG16_model.fit(train_dataset,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "                epochs=epochs,\n",
    "                validation_dataset=val_dataset,\n",
    "                validation_steps=validation_steps,\n",
    "                save_model=True,\n",
    "                model_dir='storage/Trained Models/facemask_VGG16.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
